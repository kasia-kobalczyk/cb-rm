defaults:
  - _self_
  - model: probabilistic # Your existing model default


training:
  num_epochs: 100
  lr: 1e-4
  eval_steps: 1000
  max_num_eval_steps: 10
  seed: 42
  dry_run: False
  active_learning: false
  training_mode: "sequential"   # options: joint | sequential | pretrain_joint
  cp_epochs: 2
  j_epochs: 2
  s_epochs: 2
random_init: True
loss:
  beta_concept: 1.0
  beta_kl: 0.001

save:
  project_name: active_learning
  wandb_entity: interp_rewards_RLHF
  run_name_prefix: standard_training_crm

data:
  embeddings_path: './datasets/armo-rm/embeddings/FsfairX-LLaMA3-RM-v0.1/'
  splits_path: './datasets/armo-rm/splits.csv'
  concept_labels_path: './datasets/armo-rm/concept_labels/armo-rm' 
  preference_labels_path: './datasets/armo-rm/preference_labels/armo-rm'
  batch_size: 512

resolve_model_target:
  deterministic: src.models.reward_models.BottleneckRewardModel
  probabilistic: src.models.reward_models.ProbabilisticBottleneckRewardModel

train_dataset:
  _target_: src.data.dataloaders.PreferenceDataset
  embeddings_path: ${data.embeddings_path}
  splits_path: ${data.splits_path}
  concept_labels_path: ${data.concept_labels_path}
  preference_labels_path: ${data.preference_labels_path}
  split: train

val_dataloader:
  _target_: src.data.dataloaders.get_dataloader
  cfg: ${data}
  split: val
