model:
  encoder: simple
  input_dim: 4096        
  device: cuda:0           

training:
  num_epochs: 1
  lr: 1e-4
  eval_steps: 100
  max_num_eval_steps: 10
  seed: 42
  dry_run: False         # Set to true to skip wandb logging and saving
  num_episodes: 100
  acquisition_function: uniform
  num_acquired_samples: 1024
  num_initial_samples: 2048

loss:
  beta: 0.1              # Weight for preference loss in total loss

save:
  project_name: active_pref_learning
  run_name_prefix: active_learning_test_concept
data:
  embeddings_path: './datasets/ultrafeedback/embeddings/meta-llama/Meta-Llama-3-8B/'
  splits_path: './datasets/ultrafeedback/splits.csv'
  concept_labels_path: './datasets/ultrafeedback/concept_labels/openbmb' 
  preference_labels_path: './datasets/ultrafeedback/preference_labels/openbmb_average'
  batch_size: 512
  
